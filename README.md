## About Me

I am a third-year PhD student in Systems for Machine Learning at the College of Computing, Georgia Institute of Technology. I am fortunate to be advised by Dr. Anand Iyer. My research centers on building efficient systems for large language model (LLM) inference across diverse hardware platforms. I am driven by curiosity about how to sculpt out the cognitive core of LLMs to reduce their memory footprint and latency overhead without compromising performance.â€‹

My ongoing projects and coursework aim to develop a foundational understanding of distributed systems for large-scale models. I am continually motivated by interdisciplinary nature of Systems for AI research, which supports the advancement of both technical depth and real-world impact.

### Ongoing Research
- Building an efficient system to merge LLM models in the wild to reduce their memory footprint. 
- Optimizing inference latency for Mixture of Expert models by leveraging runtime information and kernel optimization. [Paper](https://arxiv.org/abs/2411.08982)
- Efficient qubit mapping, routing and scheduling for noisy trapped-ion based quantum computers, with <a href="http://prod.tinker.cc.gatech.edu/" target="_blank"> TINKER group </a> under the supervision of <a href="https://www.ece.gatech.edu/faculty-staff-directory/tom-conte" target="_blank">Dr. Tom Conte</a>. (Master's thesis)
- Teaching neural networks how to count, under the guidance of [Dr. Sashank Varma](https://psychology.gatech.edu/sashank-varma). <a href="https://escholarship.org/uc/item/91z2p9h1" target="_blank"> Poster at CogSci'22! </a>. [Full paper accepted at CogSci'24](https://escholarship.org/content/qt5cz2v6d5/qt5cz2v6d5_noSplash_d89a45b032c07c6c0d0c5cfca8df2884.pdf)

### What's new!
- Recently passed the oral component of Systems Qualifying Examination towards the PhD requirement! (Nov 2025)


### Experience
I was a research intern at Microsoft Research with the Systems Research group (2025) and AI Frameworks team (2024) where I worked on multi-instance LLM scheduling and enriching the existing hand-written kernels for LLM inference respectively.

During summer of 2022, I explored ML compiler frameworks for [Cerebras Systems](https://www.cerebras.net/), where I had the opportunity to how make sparse attention faster on LLMs through MLIR transformation passes. Previously, I have co-founded PACE, a fitness [startup](https://create-x.gatech.edu/) with CREATE-X, aimed at remote physical therapy, leveraging deep learning and computer vision techniques. 
 
Prior to my master's, I was a Design Engineer at <a href="https://www.arm.com/" target="_blank"> Arm </a>  where I worked on physical design implementation of high performance and low power design at the sub-system level for Arm V7-V9 cores (4nm, 5nm and 11nm) through floorplan co-design for big and little cores. During my undergrad, I also worked on <a href="docs/papers/Performance_Analysis_of_a_Visible_Light_Vehicle-To-Vehicle_Wireless_Communication_System.pdf" target="_blank"> foggy channel modelling </a> for autonomous vehicles that led to Best Paper Award at IMICPW. You can find my resume <a href="docs/papers/Vima_Gupta_PhD_updated (5).pdf" target="_blank"> here </a>. 

### Interests and Hobbies

In my free time, I like to read on topics spanning mythology, psychology and ancient history. I am also huge TV show and movie buff and into casual quizzing.

### Social Networks
<p float="left">
<a href="https://scholar.google.com/citations?user=Yno2pxMAAAAJ&hl=en" target="_blank"> Google Scholar</a>
<a href="https://github.com/VimaGupta345" target="_blank"> GitHub </a>
<a href="https://www.linkedin.com/in/vima-gupta/" target="_blank"> LinkedIn </a>
</p>
